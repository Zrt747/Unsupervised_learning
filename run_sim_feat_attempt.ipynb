{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from model.network import Unsupervised_Object_Detection\n",
    "import matplotlib.pyplot as plt\n",
    "from model.utils import disp_activation, get_activation\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "im = Image.open('images/bottle0.jpg')\n",
    "target_list = {\n",
    "    'boxes': torch.tensor([[103, 212, 495, 746]], dtype=torch.float32), # xmin, ymin, xmax, ymax\n",
    "    'labels': torch.tensor([1], dtype=torch.int64),  # Class label should be int64\n",
    "    # 'image_id': torch.tensor([1], dtype=torch.int64)  # Unique ID should be int64\n",
    "}\n",
    "\n",
    "im_test = Image.open('images/bottle2.jpg')\n",
    "model = Unsupervised_Object_Detection()\n",
    "# zi, zj, za = model(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.utils import random_rotate_image, random_augment\n",
    "from model.loss_func import nt_xent_loss\n",
    "\n",
    "# Pre-training function\n",
    "def pre_train_one_image(model, image, optimizer, scheduler, loss_fn, epochs=10, patience=3, save_path='best_model.pth'):\n",
    "    best_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        new_im = random_augment(image)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        zi, zj, za = model(new_im)\n",
    "        loss = loss_fn(zi, zj, za)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step the scheduler with the current loss\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        # Check if the current loss is the best we've seen so far\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved with loss {best_loss} at epoch {best_epoch + 1}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "\n",
    "    # Optionally load the best model after training\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f\"Training completed. Best model from epoch {best_epoch + 1} loaded.\")\n",
    "\n",
    "# Define the necessary components\n",
    "loss_fn = nt_xent_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Run the pre-training loop\n",
    "pre_train_one_image(model, im, optimizer, scheduler, loss_fn, epochs=15, patience=3, save_path='best_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Load\n",
    "model = Unsupervised_Object_Detection()\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 54\u001b[0m \u001b[43mfine_tune_retinanet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretinanet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 32\u001b[0m, in \u001b[0;36mfine_tune_retinanet\u001b[0;34m(model, images, targetsOG)\u001b[0m\n\u001b[1;32m     29\u001b[0m augmented_image, augmented_target \u001b[38;5;241m=\u001b[39m random_augment_ft(images, targetsOG\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m     30\u001b[0m augmented_image \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(augmented_image)\n\u001b[0;32m---> 32\u001b[0m new_im \u001b[38;5;241m=\u001b[39m [\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_image\u001b[49m\u001b[43m)\u001b[49m]  \u001b[38;5;66;03m# List of one image tensor\u001b[39;00m\n\u001b[1;32m     33\u001b[0m new_target \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m augmented_target\u001b[38;5;241m.\u001b[39mitems()}]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# # Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torchvision/transforms/functional.py:137\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "from fine_tune_utiles import random_augment_ft\n",
    "\n",
    "def fine_tune_retinanet(model, images, targetsOG):\n",
    "\n",
    "    model.train()\n",
    "    # Freeze the backbone\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Train head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.0001)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [param for param in model.parameters() if param.requires_grad],\n",
    "        lr=0.001,\n",
    "        momentum=0.8,\n",
    "        weight_decay=0.00001,\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    num_epochs = 20\n",
    "\n",
    "    \n",
    "    new_im =  transforms.ToTensor()(images)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        augmented_image, augmented_target = random_augment_ft(images, targetsOG.copy())\n",
    "        augmented_image = transforms.ToTensor()(augmented_image)\n",
    "\n",
    "        new_im = [augmented_image]  # List of one image tensor\n",
    "        new_target = [{k: v for k, v in augmented_target.items()}]\n",
    "\n",
    "        # # Forward pass\n",
    "        loss_dict = model(new_im, new_target)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "\n",
    "        # new_im, new_target = random_augment_ft(images, targetsOG.copy())\n",
    "        # new_im = transforms.ToTensor()(new_im)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item():.4f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "fine_tune_retinanet(model.P2.retinanet,im,target_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtina = model.P2.retinanet\n",
    "rtina.eval()\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def predict_and_plot(image_path, threshold=0.5):\n",
    "    # Load image and convert to tensor\n",
    "    img = load_image(image_path)\n",
    "    img_tensor = F.to_tensor(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = rtina(img_tensor)[0]\n",
    "\n",
    "    # Filter out boxes below the confidence threshold\n",
    "    boxes = predictions['boxes']\n",
    "    scores = predictions['scores']\n",
    "    labels = predictions['labels']\n",
    "    print( scores)\n",
    "\n",
    "    selected_boxes = boxes[scores > threshold].numpy()\n",
    "\n",
    "    # Draw boxes on the image\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in selected_boxes:\n",
    "        draw.rectangle(box.tolist(), outline=\"red\", width=3)\n",
    "\n",
    "    # Display the image with bounding boxes\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "predict_and_plot('images/bottle0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils import disp_activation, get_activation\n",
    "\n",
    "tensor_im = transforms.ToTensor()(im)\n",
    "output = model.P2.retinanet.backbone(tensor_im)\n",
    "N_images = len(output)\n",
    "fig, ax = plt.subplots(1,N_images, figsize = (12,12))\n",
    "for i, (name,data) in enumerate(output.items()):\n",
    "    print(name)\n",
    "    disp_activation(data.mean(1), im,ax[i], name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_functions import livecam_test\n",
    "from capture_box_from_image import draw_rectangle\n",
    "# draw_rectangle(cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# livecam_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune_utiles import flip_boxes_horizontally,flip_boxes_vertically,rotate_image_and_boxes, plot_image_with_boxes\n",
    "import random\n",
    "img = im.copy()\n",
    "target_list = {\n",
    "    'boxes': torch.tensor([[103, 212, 495, 746]], dtype=torch.float32), # xmin, ymin, xmax, ymax\n",
    "    'labels': torch.tensor([1], dtype=torch.int64),  # Class label should be int64\n",
    "    # 'image_id': torch.tensor([1], dtype=torch.int64)  # Unique ID should be int64\n",
    "}\n",
    "\n",
    "image_width, image_height = img.size\n",
    "boxes = target_list['boxes'].clone()\n",
    "\n",
    "img = F.hflip(img)\n",
    "boxes = flip_boxes_horizontally(boxes, image_width)\n",
    "\n",
    "img = F.vflip(img)\n",
    "boxes = flip_boxes_vertically(boxes, image_height)\n",
    "\n",
    "angle = random.choice([90, 180, 270])  # Don't rotate by 0\n",
    "img, boxes = rotate_image_and_boxes(img, boxes, angle)\n",
    "        \n",
    "img = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2)(img)\n",
    "img = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.0))(img)\n",
    "        \n",
    "img = transforms.ToTensor()(img)\n",
    "img = F.to_pil_image(img)\n",
    "new_transformed_target = target_list.copy()\n",
    "new_transformed_target['boxes'] = boxes\n",
    "plot_image_with_boxes(img, new_transformed_target['boxes'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list['boxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, new_target = random_augment_ft(im, target_list)\n",
    "images = transforms.ToTensor()(images)\n",
    "plot_image_with_boxes(F.to_pil_image(images), new_target['boxes'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
