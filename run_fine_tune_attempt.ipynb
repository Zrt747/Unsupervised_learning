{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from network import Unsupervised_Object_Detection\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import disp_activation, get_activation\n",
    "\n",
    "im = Image.open('bottle1.jpg')\n",
    "target_list = {\n",
    "    'boxes': torch.tensor([[463.0, 20.0, 666.0, 588.0]], dtype=torch.float32),  # Ensure float32 type\n",
    "    'labels': torch.tensor([1], dtype=torch.int64),  # Class label should be int64\n",
    "    # 'image_id': torch.tensor([1], dtype=torch.int64)  # Unique ID should be int64\n",
    "}\n",
    "im_test = Image.open('bottle2.jpg')\n",
    "model = Unsupervised_Object_Detection()\n",
    "# zi, zj, za = model(im)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erezshani/Projects/Unsupervised_costractive/pipelines.py:123: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  return torch.tensor(anchor_negatives, device=fpn_output.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.15993595123291\n",
      "Epoch 2/10, Loss: 1.2067207098007202\n",
      "Epoch 3/10, Loss: 1.0558346509933472\n",
      "Epoch 4/10, Loss: 0.11328437179327011\n",
      "Epoch 5/10, Loss: 0.11015941947698593\n",
      "Epoch 6/10, Loss: 0.08154162764549255\n",
      "Epoch 7/10, Loss: 0.06114836782217026\n",
      "Epoch 8/10, Loss: 0.25982779264450073\n",
      "Epoch 9/10, Loss: 0.017796523869037628\n",
      "Epoch 10/10, Loss: 0.01975332200527191\n"
     ]
    }
   ],
   "source": [
    "from utils import random_rotate_image, random_augment\n",
    "from loss_func import nt_xent_loss\n",
    "\n",
    "# Pre-training function\n",
    "def pre_train_one_image(model, image, optimizer,scheduler,loss, epochs=10):\n",
    "    \n",
    "    new_im = image\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        zi, zj, za = model(new_im)\n",
    "        loss = nt_xent_loss(zi,zj,za)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
    "        # new_im = random_rotate_image(image)\n",
    "        new_im = random_augment(image)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "loss = nt_xent_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "pre_train_one_image(model,im,optimizer,scheduler, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune_utiles import random_augment_ft, plot_image_with_boxes\n",
    "target_list = {\n",
    "    'boxes': torch.tensor([[463.0, 20.0, 666.0, 588.0]], dtype=torch.float32),  # Ensure float32 type\n",
    "    'labels': torch.tensor([1], dtype=torch.int64),  # Class label should be int64\n",
    "    # 'image_id': torch.tensor([1], dtype=torch.int64)  # Unique ID should be int64\n",
    "}\n",
    "new_im, new_target = random_augment_ft(im, target_list)\n",
    "plot_image_with_boxes(new_im, new_target['boxes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for binary classification.\n",
    "        \n",
    "        :param alpha: Balancing factor (usually between 0.25 and 1).\n",
    "        :param gamma: Focusing parameter (usually 2).\n",
    "        :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of the loss function.\n",
    "        \n",
    "        :param inputs: Predicted logits (before sigmoid), shape (N, *).\n",
    "        :param targets: Ground truth binary labels (0 or 1), shape (N, *).\n",
    "        :return: Loss value.\n",
    "        \"\"\"\n",
    "        # Convert targets to the same shape as inputs\n",
    "        targets = targets.view(-1, 1)\n",
    "        inputs= inputs.view(-1, 1)\n",
    "        \n",
    "        # Apply sigmoid to get the probabilities\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        \n",
    "        # Get probabilities after sigmoid\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 59\u001b[0m \u001b[43mfine_tune_retinanet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretinanet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mfine_tune_retinanet\u001b[0;34m(model, images, targets)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(new_im, targets)  \u001b[38;5;66;03m# Get model outputs (logits)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m classification_loss \u001b[38;5;241m=\u001b[39m \u001b[43mfocal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m regression_loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox_regression\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloss(targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Assuming bbox regression loss\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Combine the losses\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mFocalLoss.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     28\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Apply sigmoid to get the probabilities\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m BCE_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Get probabilities after sigmoid\u001b[39;00m\n\u001b[1;32m     34\u001b[0m pt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mBCE_loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.10/site-packages/torch/nn/functional.py:3162\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "from fine_tune_utiles import random_augment_ft\n",
    "\n",
    "focal_loss = FocalLoss(alpha=1, gamma=2)\n",
    "def fine_tune_retinanet(model, images, targets):\n",
    "\n",
    "    model.train()\n",
    "    # Freeze the backbone\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Train head\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.0001)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [param for param in model.parameters() if param.requires_grad],\n",
    "        lr=0.001,\n",
    "        momentum=0.8,\n",
    "        weight_decay=0.00001,\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    num_epochs = 8\n",
    "\n",
    "    \n",
    "    new_im =  transforms.ToTensor()(images)\n",
    "    new_target = targets\n",
    "    for epoch in range(num_epochs):\n",
    "        # new_im = [new_im]#.to('cuda')\n",
    "        new_im = [new_im]\n",
    "        targets = [{k: v for k, v in new_target.items()}]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(new_im, targets)  # Get model outputs (logits)\n",
    "\n",
    "        classification_loss = focal_loss(outputs['classification'], targets[0]['labels'].squeeze())\n",
    "        regression_loss = outputs['bbox_regression'].loss(targets['boxes'])  # Assuming bbox regression loss\n",
    "        \n",
    "                # Combine the losses\n",
    "        losses = classification_loss + regression_loss\n",
    "\n",
    "        # # Forward pass\n",
    "        # loss_dict = model(new_im, targets)\n",
    "        # losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # new_im = random_augment(images)\n",
    "        new_im, new_target = random_augment_ft(images, target_list)\n",
    "        new_im = transforms.ToTensor()(new_im)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {losses.item():.4f}\")\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "fine_tune_retinanet(model.P2.retinanet,im,target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model = Unsupervised_Object_Detection()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fine_tune_utiles import plot_predictions\n",
    "\n",
    "# Example usage\n",
    "img = transforms.ToTensor()(im)\n",
    "img = img.unsqueeze(0) # needs to be 4d!\n",
    "model.P2.retinanet.eval()\n",
    "predictions = model.P2.retinanet(img)\n",
    "# print(predictions[0]['scores'])\n",
    "plot_predictions(im, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "img = transforms.ToTensor()(im_test)\n",
    "img = img.unsqueeze(0) # needs to be 4d!\n",
    "model.P2.retinanet.eval()\n",
    "predictions = model.P2.retinanet(img)\n",
    "# print(predictions[0]['scores'])\n",
    "plot_predictions(im_test, predictions,threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "\n",
    "def get_activation(activation, img):\n",
    "    # w,h = img.size\n",
    "    h,w = img.size\n",
    "\n",
    "    # Normalize the Grad-CAM\n",
    "    grad_cam = torch.nn.functional.relu(activation)\n",
    "    grad_cam = torch.nn.functional.interpolate(grad_cam.unsqueeze(0), size=(w,h), mode='bilinear', align_corners=False)\n",
    "    grad_cam = grad_cam.detach().squeeze().cpu().numpy()\n",
    "    grad_cam = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min())\n",
    "    return grad_cam\n",
    "\n",
    "w,h = im.size\n",
    "total_activation = np.zeros(shape=(h,w))\n",
    "tensor_im = transforms.ToTensor()(im)\n",
    "output = model.P2.retinanet.backbone(tensor_im)\n",
    "for i, (name,data) in enumerate(output.items()):\n",
    "    total_activation += get_activation(data.mean(1), im)\n",
    "total_activation = min_max(total_activation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranpose_im = np.array(im.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "plt.imshow(tranpose_im)\n",
    "mask = np.stack([total_activation] * 3, axis=-1) \n",
    "a = mask * tranpose_im\n",
    "a = np.clip(a, 0, 255).astype(np.uint8)\n",
    "# plt.imshow(a)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(total_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(total_activation>0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import disp_activation, get_activation\n",
    "\n",
    "tensor_im = transforms.ToTensor()(im)\n",
    "output = model.P2.retinanet.backbone(tensor_im)\n",
    "N_images = len(output)\n",
    "fig, ax = plt.subplots(1,N_images, figsize = (12,12))\n",
    "for i, (name,data) in enumerate(output.items()):\n",
    "    print(name)\n",
    "    disp_activation(data.mean(1), im,ax[i], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_im = transforms.ToTensor()(im_test)\n",
    "output = model.P2.retinanet.backbone(tensor_im)\n",
    "N_images = len(output)\n",
    "fig, ax = plt.subplots(1,N_images, figsize = (12,12))\n",
    "for i, (name,data) in enumerate(output.items()):\n",
    "    print(name)\n",
    "    disp_activation(data.mean(1), im_test,ax[i], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing_functions import livecam_test\n",
    "from capture_box_from_image import draw_rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
